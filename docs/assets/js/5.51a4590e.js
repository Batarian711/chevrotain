(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{57:function(t,s,a){"use strict";a.r(s);var n=a(0),e=Object(n.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("div",{staticClass:"content"},[a("h1",{attrs:{id:"tutorial-lexer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tutorial-lexer","aria-hidden":"true"}},[t._v("#")]),t._v(" Tutorial - Lexer")]),a("h3",{attrs:{id:"tldr"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tldr","aria-hidden":"true"}},[t._v("#")]),t._v(" TLDR")]),a("p",[a("a",{attrs:{href:"https://github.com/SAP/chevrotain/tree/master/examples/tutorial/step1_lexing",target:"_blank",rel:"noopener noreferrer"}},[t._v("Run and Debug the source code")]),t._v(".")]),a("h2",{attrs:{id:"introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#introduction","aria-hidden":"true"}},[t._v("#")]),t._v(" Introduction")]),a("p",[t._v("In This tutorial we will implement a Lexer for a simple SQL Select statement language:")]),a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" column1 "),a("span",{attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" table2\n"),a("span",{attrs:{class:"token keyword"}},[t._v("SELECT")]),t._v(" name"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" age "),a("span",{attrs:{class:"token keyword"}},[t._v("FROM")]),t._v(" persons "),a("span",{attrs:{class:"token keyword"}},[t._v("WHERE")]),t._v(" age "),a("span",{attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),a("span",{attrs:{class:"token number"}},[t._v("100")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("\n")])]),a("p",[t._v("A Lexer transforms a string input into a "),a("a",{attrs:{href:"https://sap.github.io/chevrotain/documentation/3_1_0/interfaces/itoken.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Token")]),t._v(" vector.\nChevrotain has a built in Lexer engine based on Javascript Regular Expressions.")]),a("h2",{attrs:{id:"our-first-token"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#our-first-token","aria-hidden":"true"}},[t._v("#")]),t._v(" Our First Token")]),a("p",[t._v('To use the Chevrotain lexer the Tokens must first be defined.\nLets examine the definition for a "FROM" Token:')]),a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" createToken "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createToken\n"),a("span",{attrs:{class:"token comment"}},[t._v("// using createToken API")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" From "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"From"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/FROM/")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),a("p",[t._v("There is nothing much to it. The pattern property is a RegExp which will be used when splitting up the input string\ninto separate Tokens.")]),a("p",[t._v("We will use the "),a("a",{attrs:{href:"https://sap.github.io/chevrotain/documentation/3_1_0/globals.html#createtoken",target:"_blank",rel:"noopener noreferrer"}},[a("strong",[t._v("createToken")]),t._v(" API")]),t._v("\nin the rest of tutorial because ES2015 has no support for static fields.")]),a("h2",{attrs:{id:"more-complex-tokens"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#more-complex-tokens","aria-hidden":"true"}},[t._v("#")]),t._v(" More complex Tokens")]),a("p",[t._v("How can we define Tokens for Identifiers or Integers?")]),a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" Identifier "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"Identifier"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/\\w+/")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" Integer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"Integer"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/0|[1-9]\\d+/")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),a("h2",{attrs:{id:"skipping-tokens"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#skipping-tokens","aria-hidden":"true"}},[t._v("#")]),t._v(" Skipping Tokens")]),a("p",[t._v("The obvious use case in this language (and many others) is "),a("strong",[t._v("whitespace")]),t._v(". skipping certain Tokens is easily\naccomplished by marking them with the SKIP group.")]),a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" WhiteSpace "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"WhiteSpace"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/\\s+/")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    group"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Lexer"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token constant"}},[t._v("SKIPPED")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    line_breaks"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token boolean"}},[t._v("true")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),a("ul",[a("li",[t._v("Note that we used the "),a("strong",[t._v("line_breaks")]),t._v(" property to flag that the WhiteSpace token may contain line terminators.\nThis is needed by the lexer to keep track of line and column numbers.")])]),a("h2",{attrs:{id:"all-our-tokens"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#all-our-tokens","aria-hidden":"true"}},[t._v("#")]),t._v(" All Our Tokens")]),a("p",[t._v('Lets examine all the needed Tokens definitions"')]),a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" Identifier "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"Identifier"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/[a-zA-Z]\\w*/")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v('// We specify the "longer_alt" property to resolve keywords vs identifiers ambiguity.')]),t._v("\n"),a("span",{attrs:{class:"token comment"}},[t._v("// See: https://github.com/SAP/chevrotain/blob/master/examples/lexer/keywords_vs_identifiers/keywords_vs_identifiers.js")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" Select "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"Select"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/SELECT/")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    longer_alt"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Identifier\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" From "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"From"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/FROM/")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    longer_alt"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Identifier\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" Where "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"Where"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/WHERE/")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    longer_alt"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" Identifier\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" Comma "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"Comma"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/,/")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" Integer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"Integer"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/0|[1-9]\\d*/")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" GreaterThan "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"GreaterThan"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/>/")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" LessThan "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"LessThan"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/</")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{attrs:{class:"token keyword"}},[t._v("const")]),t._v(" WhiteSpace "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token function"}},[t._v("createToken")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    name"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"WhiteSpace"')]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    pattern"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token regex"}},[t._v("/\\s+/")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    group"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" chevrotain"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Lexer"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token constant"}},[t._v("SKIPPED")]),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    line_breaks"),a("span",{attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),a("span",{attrs:{class:"token boolean"}},[t._v("true")]),t._v("\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),a("h2",{attrs:{id:"creating-the-lexer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#creating-the-lexer","aria-hidden":"true"}},[t._v("#")]),t._v(" Creating The Lexer")]),a("p",[t._v("We now have Token definitions, but how do we create a Lexer from these?")]),a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token comment"}},[t._v("// note we are placing WhiteSpace first as it is very common thus it will speed up the lexer.")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("let")]),t._v(" allTokens "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    WhiteSpace"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v('// "keywords" appear before the Identifier')]),t._v("\n    Select"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    From"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    Where"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    Comma"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),a("span",{attrs:{class:"token comment"}},[t._v("// The Identifier must appear after the keywords because all keywords are valid identifiers.")]),t._v("\n    Identifier"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    Integer"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    GreaterThan"),a("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    LessThan\n"),a("span",{attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("let")]),t._v(" SelectLexer "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),a("span",{attrs:{class:"token class-name"}},[t._v("Lexer")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("allTokens"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),a("p",[t._v("Note that:")]),a("ul",[a("li",[a("p",[t._v("The "),a("strong",[t._v("order")]),t._v(" of Token definitions passed to the Lexer is "),a("strong",[t._v("important")]),t._v(".\nThe first PATTERN to match will be chosen not the longest.")]),a("ul",[a("li",[t._v("See how to resolve "),a("a",{attrs:{href:"https://github.com/SAP/chevrotain/blob/master/examples/lexer/keywords_vs_identifiers/keywords_vs_identifiers.js",target:"_blank",rel:"noopener noreferrer"}},[t._v("Keywords vs Identifiers")])])])]),a("li",[a("p",[t._v("The Chevrotain Lexer is "),a("strong",[t._v("stateless")]),t._v(", thus only a "),a("strong",[t._v("single one per grammar")]),t._v(" should ever be created.")])])]),a("h2",{attrs:{id:"using-the-lexer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#using-the-lexer","aria-hidden":"true"}},[t._v("#")]),t._v(" Using The Lexer")]),a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{attrs:{class:"token keyword"}},[t._v("let")]),t._v(" inputText "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{attrs:{class:"token string"}},[t._v('"SELECT column1 FROM table2"')]),t._v("\n"),a("span",{attrs:{class:"token keyword"}},[t._v("let")]),t._v(" lexingResult "),a("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" SelectLexer"),a("span",{attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{attrs:{class:"token function"}},[t._v("tokenize")]),a("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("inputText"),a("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),a("p",[t._v("The Lexing Result will contain:")]),a("ol",[a("li",[t._v("A Token Vector.")]),a("li",[t._v("the lexing errors (if any were encountered)")]),a("li",[t._v("And other "),a("a",{attrs:{href:"https://github.com/SAP/chevrotain/blob/master/examples/lexer/token_groups/token_groups.js",target:"_blank",rel:"noopener noreferrer"}},[t._v("Token groups")]),t._v(" (if grouping was used)")])])])}],!1,null,null,null);s.default=e.exports}}]);
(window.webpackJsonp=window.webpackJsonp||[]).push([[7],{54:function(t,e,s){"use strict";s.r(e);var n=s(0),a=Object(n.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var t=this,e=t.$createElement,s=t._self._c||e;return s("div",{staticClass:"content"},[s("h1",{attrs:{id:"syntactic-content-assist"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#syntactic-content-assist","aria-hidden":"true"}},[t._v("#")]),t._v(" Syntactic Content Assist")]),s("h3",{attrs:{id:"tldr"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tldr","aria-hidden":"true"}},[t._v("#")]),t._v(" TLDR")]),s("p",[t._v("See: "),s("a",{attrs:{href:"https://github.com/SAP/chevrotain/blob/master/examples/parser/content_assist/official_feature_content_assist.js",target:"_blank",rel:"noopener noreferrer"}},[s("strong",[t._v("Runnable example")])]),t._v(" for quick starting.")]),s("h2",{attrs:{id:"introduction"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#introduction","aria-hidden":"true"}},[t._v("#")]),t._v(" Introduction")]),s("p",[t._v("Chevrotain provides Syntactic Content assist Capabilities.\nThese can be accessed via the "),s("a",{attrs:{href:"https://sap.github.io/chevrotain/documentation/3_2_1/classes/parser.html#computecontentassist",target:"_blank",rel:"noopener noreferrer"}},[s("strong",[t._v("computeContentAssist")])]),t._v(" method.")]),s("p",[t._v("Note that this feature "),s("strong",[t._v("only")]),t._v(" provides syntactic suggestions (meaning next possible token types) "),s("strong",[t._v("not")]),t._v(' semantic suggestions.\nIt could be used as a building block in a semantic suggestions provider, but it cannot do this on "its own".')]),s("p",[t._v("Also note that this feature is implemented as a "),s("strong",[t._v("separate")]),t._v(" interpreted backtracking parser,\ncompletely unrelated to the normal parsing flow except for using the same internal grammar representation.")]),s("h2",{attrs:{id:"limitations"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#limitations","aria-hidden":"true"}},[t._v("#")]),t._v(" Limitations")]),s("p",[t._v("This causes several important limitations:")]),s("ul",[s("li",[s("p",[s("strong",[t._v("Performance")]),t._v(": The Content assist feature is about x10 times slower than the normal parsing flow.")])]),s("li",[s("p",[t._v("No Embedded actions will be performed.")])]),s("li",[s("p",[t._v("Error Recovery is unsupported")])]),s("li",[s("p",[t._v("Gates / Predicates are unsupported.")])])]),s("p",[t._v("These limitations may seem daunting at first, but should not cause great problems in actual practice.\nThe following sections will discuss each limitation in details.")]),s("h2",{attrs:{id:"performance"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#performance","aria-hidden":"true"}},[t._v("#")]),t._v(" Performance.")]),s("p",[t._v("An order of magnitude slower performance may at first sound like a horrible thing.\nLets put this in perspective for relevant use cases:")]),s("ul",[s("li",[s("p",[t._v("Being an order of magnitude slower also means approximately the same speed as Jison.")]),s("ul",[s("li",[t._v("Tested on Chrome 68, See: "),s("a",{attrs:{href:"https://sap.github.io/chevrotain/performance/",target:"_blank",rel:"noopener noreferrer"}},[t._v("performance benchmark")]),t._v(".")])])]),s("li",[s("p",[s("strong",[t._v("Smaller input Size 1")]),t._v(": Content Assist is requested for an offset inside a text, this means that on average only half the text input\nwill have to be parsed. Suddenly the problem is halved...")])]),s("li",[s("p",[s("strong",[t._v("Smaller input size 2")]),t._v(": Syntactic Content Assist is requested for a single text input(file), while standard parsing flow may need\nto handle dozens or hundreds of inputs at once.")])]),s("li",[s("p",[s("strong",[t._v("Less work")]),t._v(": The ~x10 performance difference was measured when comparing pure grammars without any semantics.\nIn a "),s("strong",[t._v("real world")]),t._v(" scenario the difference will be smaller as the regular parser will have semantics (CST creation / embedded actions).\nThoses semantics have their own runtime cost while the content assist parser will "),s("strong",[t._v("always")]),t._v(" remain a pure grammar.")])]),s("li",[s("p",[s("strong",[t._v("Smart beats fast")]),t._v(": Content Assist is normally used in a code editor. A code editor should be by definition\n"),s("strong",[t._v("incremental")]),t._v(" as it does not matter how smart the error recovery is or how fast the parser, re-parsing a whole\nfile for every single changed character will simply "),s("strong",[t._v("not scale")]),t._v(", both from a performance perspective and from the requirement\nof handling partially invalid inputs. What this means is that if a code editor is already (as it should be) incremental.\nIt could invoke the call to "),s("strong",[t._v("computeContentAssist")]),t._v(" on a subset of the input as well. This subset could easily\nbe "),s("strong",[t._v("several")]),t._v(" orders of magnitudes smaller, thus all performance concerns are resolved.")]),s("ul",[s("li",[s("p",[t._v("Example: Imagine a 1,000 lines JavaScript file where a single 10 lines function is being edited and content assist\nis requested inside that small function.")]),s("pre",{pre:!0,attrs:{class:"language-javascript"}},[s("code",[s("span",{attrs:{class:"token comment"}},[t._v("// line 1")]),t._v("\n"),s("span",{attrs:{class:"token comment"}},[t._v("// .")]),t._v("\n"),s("span",{attrs:{class:"token comment"}},[t._v("// .")]),t._v("\n"),s("span",{attrs:{class:"token comment"}},[t._v("// .")]),t._v("\n"),s("span",{attrs:{class:"token comment"}},[t._v("// line 600")]),t._v("\n"),s("span",{attrs:{class:"token keyword"}},[t._v("function")]),t._v(" "),s("span",{attrs:{class:"token function"}},[t._v("foo")]),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{attrs:{class:"token comment"}},[t._v("// content assist requested somewhere in this function")]),t._v("\n"),s("span",{attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{attrs:{class:"token comment"}},[t._v("// line 610")]),t._v("\n"),s("span",{attrs:{class:"token comment"}},[t._v("// .")]),t._v("\n"),s("span",{attrs:{class:"token comment"}},[t._v("// .")]),t._v("\n"),s("span",{attrs:{class:"token comment"}},[t._v("// .")]),t._v("\n"),s("span",{attrs:{class:"token comment"}},[t._v("// line 1000")]),t._v("\n")])]),s("p",[t._v("There is no need no re-parse the whole file, Instead only the text of that function should be sent\nTo "),s("strong",[t._v("computeContentAssist")]),t._v(' and the "startRule" should be "functionDeclaration". Therefore only ~10 lines\nof text will have to be re-parsed to provide syntactic content assist.\nThis is a 1/100 difference in input size which is '),s("strong",[t._v("two orders of magnitude")]),t._v(" smaller.")])])])])]),s("h2",{attrs:{id:"semantics-fault-tolerance"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#semantics-fault-tolerance","aria-hidden":"true"}},[t._v("#")]),t._v(" Semantics & Fault tolerance")]),s("p",[t._v("No support for semantic actions and Error Recovery.")]),s("p",[t._v("Once again, Content assist is often used in a code Editor's context.\nsemantic actions will normally only output useful results for "),s("strong",[t._v("valid parts")]),t._v(' of the input.\nError recovery can help with invalid inputs by performing small automatic fixes to the input and more often by completely\nskipping (re-syncing) parts of the input until a "valid" section is encountered.')]),s("p",[t._v("The problem is that usually the code area where content assist is requested is also currently being "),s("strong",[t._v("heavily")]),t._v(" modified by the user\nand is unlikely to able to be successfully parsed or automatically recovered(fixed), instead it will probably be skipped (re-synced) entirely.")]),s("p",[t._v("What this means is that for input areas that are currently being edited (or even written from scratch) by the user\nsemantic actions and error recovery are less relevant anyhow. And if as described in the previous section an incremental approach\nto using the content assist will also resolve the issue in which the content assist position follows a syntax error.")]),s("p",[t._v("Example:")]),s("pre",{pre:!0,attrs:{class:"language-javascript"}},[s("code",[s("span",{attrs:{class:"token keyword"}},[t._v("let")]),t._v(" three "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token comment"}},[t._v("// syntax error, missing expression")]),t._v("\n"),s("span",{attrs:{class:"token keyword"}},[t._v("let")]),t._v(" five "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("5")]),t._v("\n"),s("span",{attrs:{class:"token keyword"}},[t._v("let")]),t._v(" six "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("1")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{attrs:{class:"token comment"}},[t._v("// <-- content assist requested after the '+'")]),t._v("\n"),s("span",{attrs:{class:"token keyword"}},[t._v("return")]),t._v(" Math"),s("span",{attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{attrs:{class:"token function"}},[t._v("max")]),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("five"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" six"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("ol",[s("li",[s("p",[t._v("The first and third statements are syntactically invalid.")])]),s("li",[s("p",[t._v("Error recovery is likely to re-sync to the following statements instead of resolving this with single token insertion / deletion.")])]),s("li",[s("p",[t._v("Therefore the results of embedded actions on these statements will not be useful.")])]),s("li",[s("p",[t._v('Content assist is request in the third statement after the "+" operator.')])]),s("li",[s("p",[t._v('If we try to send the whole text to request content assist suggestions until the offset after the "+" operator\nNo suggestions will be found due to the syntax error on the first statement.')])]),s("li",[s("p",[t._v('However if we only send the text of the third statement ("let six = 1 + ") content assist will work successfully.')])])]),s("h2",{attrs:{id:"gates-predicates"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#gates-predicates","aria-hidden":"true"}},[t._v("#")]),t._v(" Gates / Predicates.")]),s("p",[t._v("Gates / Predicates behave is limiters of the available grammar. These constructs are not often used in grammars\nwhich reduces the severity of this limitation.")]),s("p",[t._v("But more importantly this "),s("strong",[t._v("does not mean that grammars using Gates cannot use the content assist functionality")]),t._v(".\nRather that in some "),s("strong",[t._v("pathological edge cases")]),t._v(" the suggestions may not be completely valid.\nIn that case additional post processing logic may need to be added to farther filter the suggestions.")]),s("p",[t._v('In details: the content assist parser is a "backtracking" Parser,\nwhich means it will try '),s("strong",[t._v("all")]),t._v(" the paths until it finds a valid one.\nIt disregards the max token lookahead constraints of the base Chevrotain parser.\nTherefor for a grammar path that has been disabled by a Gate / Predicate to be offered as content assist suggestion:\nNot only does the fixed tokens lookahead have to match at the Gate's position.\nThe "),s("strong",[t._v("remaining input")]),t._v(" following the Gate's position must "),s("strong",[t._v("also")]),t._v(" match.\nThis is a much stronger condition (K tokens vs dozens/hundreds/infinity) thus making this issue a very minor one.")])])}],!1,null,null,null);e.default=a.exports}}]);
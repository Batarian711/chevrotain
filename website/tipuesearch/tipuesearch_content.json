{"pages":[{"title":"Chevrotain","text":"Chevrotain is a very fast and feature rich Parser Building Toolkit for JavaScript. It can be used to build parsers\/compilers\/interpreters for various use cases ranging from simple configuration files, to full fledged programing languages. A more in depth description of Chevrotain can be found in this great article on: Parsing in JavaScript: Tools and Libraries. It is important to note that Chevrotain is NOT a parser generator. It solves the same kind of problems as a parser generator, just without any code generation. Chevrotain Grammars are pure code which can be created\/debugged\/edited as any other pure code without requiring any new tools or processes. TLDR Online Playground Performance benchmark","tags":"","url":"index.html"},{"title":"step1 lexing","text":"Tutorial Step 1 - Building a Lexer. ---&gt; Source Code for this step &lt;--- On code samples: The tutorial uses ES2015+ syntax. See examples of using Chevrotain in other implementation languages. Introduction: In This tutorial we will implement a Lexer for a simple SQL Select statement language: SELECT column1 FROM table2 SELECT name, age FROM persons WHERE age &gt; 100 ... A Lexer transforms a string input into a Token vector. Chevrotain has a built in Lexer engine based on Javascript Regular Expressions. To use the Chevrotain lexer the Tokens must first be defined. Lets examine the definition for a &quot;FROM&quot; Token: const createToken = chevrotain.createToken \/\/ using createToken API const From = createToken({name: &quot;From&quot;, pattern: \/FROM\/}); \/\/ Using Class syntax class From extends Token {} \/\/ manually creating static fields as those are not yet supported in ES2015 From.PATTERN = \/FROM\/ There is nothing much to it. The pattern\/PATTERN property is a RegExp which will be used when splitting up the input string into separate Tokens. We will use the createToken API in the rest of tutorial because ES2015 has no support for static fields. What about a slightly more complex Tokens? How can we define Tokens for Identifiers or Integers? const Identifier = createToken({name: &quot;Identifier&quot;, pattern: \/\\w+\/}); const Integer = createToken({name: &quot;Integer&quot;, pattern: \/0|[1-9]\\d+\/}); What about skipping certain Tokens? The obvious use case in this language (and many others) is whitespace. skipping certain Tokens is easily accomplished by marking them with the SKIP group. const WhiteSpace = createToken({ name: &quot;WhiteSpace&quot;, pattern: \/\\s+\/, group: chevrotain.Lexer.SKIPPED, line_breaks: true }); Note that we used the line_breaks property to flag that the WhiteSpace token may contain line terminators. This is needed by the lexer to keep track of line and column numbers. Let us define all our nine Tokens: const Identifier = createToken({name: &quot;Identifier&quot;, pattern: \/[a-zA-Z]\\w*\/}); \/\/ We specify the &quot;longer_alt&quot; property to resolve keywords vs identifiers ambiguity. \/\/ See: https:\/\/github.com\/SAP\/chevrotain\/blob\/master\/examples\/lexer\/keywords_vs_identifiers\/keywords_vs_identifiers.js const Select = createToken({name: &quot;Select&quot;, pattern: \/SELECT\/, longer_alt: Identifier}); const From = createToken({name: &quot;From&quot;, pattern: \/FROM\/, longer_alt: Identifier}); const Where = createToken({name: &quot;Where&quot;, pattern: \/WHERE\/, longer_alt: Identifier}); const Comma = createToken({name: &quot;Comma&quot;, pattern: \/,\/}); const Integer = createToken({name: &quot;Integer&quot;, pattern: \/0|[1-9]\\d*\/}); const GreaterThan = createToken({name: &quot;GreaterThan&quot;, pattern: \/&gt;\/}); const LessThan = createToken({name: &quot;LessThan&quot;, pattern: \/&lt;\/}); const WhiteSpace = createToken({ name: &quot;WhiteSpace&quot;, pattern: \/\\s+\/, group: chevrotain.Lexer.SKIPPED, line_breaks: true }); All right, we have Token definitions, how do we use them to create a Lexer? \/\/ note we are placing WhiteSpace first as it is very common thus it will speed up the lexer. let allTokens = [ WhiteSpace, \/\/ &quot;keywords&quot; appear before the Identifier Select, From, Where, Comma, \/\/ The Identifier must appear after the keywords because all keywords are valid identifiers. Identifier, Integer, GreaterThan, LessThan] let SelectLexer = new Lexer(allTokens); Note that: The order of Token definitions passed to the Lexer is important. The first PATTERN to match will be chosen not the longest. See how to resolve Keywords vs Identifiers The Chevrotain Lexer is stateless, thus only a single one per grammar should ever be created. But how do we actually use this lexer? let inputText = &quot;SELECT column1 FROM table2&quot; let lexingResult = SelectLexer.tokenize(inputText) The Lexing Result will contain: A Token Vector. the lexing errors (if any were encountered) And other Token groups (if grouping was used) What is Next? Run &amp; Debug the source code of this tutorial step. Move to the next step: Step 2 - Parsing.","tags":"","url":"Tutorial\/step1_lexing.html"},{"title":"step2 parsing","text":"Previous tutorial step - Step 1 - Lexing Tutorial Step 2 - Building a Parser. ---&gt; Source Code for this step &lt;--- On code samples: The tutorial uses ES2015+ syntax. See examples of using Chevrotain in other implementation languages. Introduction: In this tutorial we will implement a Parser for a simple SQL Select statement language introduced in the previous tutorial step. The grammar for our language: selectStatement : selectClause fromClause (whereClause)? selectClause : &quot;SELECT&quot; Identifier (&quot;,&quot; Identifier)* fromClause : &quot;FROM&quot; Identifier whereClause : &quot;WHERE&quot; expression expression : atomicExpression relationalOperator atomicExpression atomicExpression : Integer | Identifier relationalOperator : &quot;&gt;&quot; | &quot;&lt;&quot; A Chevrotain Parser analyses an IToken vector that conforms to some grammar. The grammar is defined using the parsing DSL, which includes the following methods. CONSUME - 'eat' a Token. SUBRULE - reference to another rule. OR - Alternation OPTION - optional production. MANY - repetition zero or more. AT_LEAST_ONE - repetition one or more. MANY_SEP - repetition (zero or more) with a separator between any two items AT_LEAST_ONE_SEP - repetition (one or more) with a separator between any two items Let's implement our first grammar rule. \/\/ selectStatement \/\/ : selectClause fromClause (whereClause)?; const $ = this $.RULE(&quot;selectStatement&quot;, () =&gt; { $.SUBRULE($.selectClause) $.SUBRULE($.fromClause) $.OPTION(() =&gt; { $.SUBRULE($.whereClause) }) }) Fairly straight forward translation: Non-Terminals --&gt; SUBRULE &quot;?&quot; --&gt; OPTION What is 'this' in this context? where do we write the grammar rules? Each grammar rule is a property of a class that extends chevrotain.Parser. const { Parser } = require(&quot;chevrotain&quot;) const allTokens = [WhiteSpace, Select, From, Where, Comma, Identifier, Integer, GreaterThan, LessThan] class SelectParser extends Parser { \/** * @param {IToken[]} input *\/ constructor(input) { super(input, allTokens) const $ = this; $.RULE(&quot;selectStatement&quot;, () =&gt; { $.SUBRULE($.selectClause) $.SUBRULE($.fromClause) $.OPTION(() =&gt; { $.SUBRULE($.whereClause) }) }) Parser.performSelfAnalysis(this) } } Important to note that: The super invocation has an array of the Tokens as the second parameter. This is the same array we used to define the Lexer and it is used to define the Parser's vocabulary. The static method Parser.performSelfAnalysis must be invoked at the end of the constructor. This is where much of the 'secret sauce' happens, including creating the inner grammar representation and performing static checks on the grammar. Let's look at two more grammar rule, this time with repetition and alternation. $.RULE(&quot;selectClause&quot;, () =&gt; { $.CONSUME(Select); $.AT_LEAST_ONE_SEP({SEP: Comma, DEF: () =&gt; { $.CONSUME(Identifier); }}) }) \/\/ atomicExpression \/\/ : INTEGER | IDENTIFIER $.RULE(&quot;atomicExpression&quot;, () =&gt; { $.OR([ {ALT: () =&gt; $.CONSUME(Integer)}, {ALT: () =&gt; $.CONSUME(Identifier)} ]) }) How can the Parser be debugged? The grammar rules above do not only define the grammar, they are also the code that will be run during parsing. This means that you can debug the parser simply by adding a break point in the grammar. \/\/ selectClause \/\/ : &quot;SELECT&quot; IDENTIFIER (&quot;,&quot; IDENTIFIER)*; $.RULE(&quot;selectClause&quot;, () =&gt; { $.CONSUME(Select) \/\/ Can be debugged directly! no code generation. debugger; $.AT_LEAST_ONE_SEP({SEP: Comma, DEF: () =&gt; { $.CONSUME(Identifier) }}) }) There do not exist two different representations for the grammar and the runnable implementation (for example, grammar file vs generated code in the case of parser generators). Again, please note that Chevrotain is NOT a parser generator. Extra details can be found in the FAQ. But how does it work? (skip if you don't care :) ) The code above will be executed as is. Yet we have not implemented a lookahead function to choose between the two OR alternatives ( INTEGER | IDENTIFIER), nor have we implemented logic to identify the next iteration for (&quot;,&quot; IDENTIFIER)*. So how does it work? The answer is the 'secret sauce' of Chevrotain: $.RULE will both: Analyse (using Function.toString) the implementation passed to it and construct a representation of the grammar in memory. Wrap the implementation passed to it in logic for running the Parser (fault tolerance\/rule stacks\/...) Parser.performSelfAnalysis(this) will finish 'compiling' the grammar representation (name resolution\/static analysis) So when the parser needs to choose between the two alternatives: $.OR([ {ALT: () =&gt; { $.CONSUME(Integer)}}, {ALT: () =&gt; { $.CONSUME(Identifier)}} ]); It is aware of: Where it is (OR [1] INSIDE_RULE [A] INSIDE_RULE [B] ...) What Tokens can come next for each alternative, as it &quot;is aware&quot; of the whole grammar representation. Thus the parser can dynamically create (and cache) the lookahead function to choose between the two alternatives. The same applies for any grammar rule where the parser has a choice, and even in some where there is no choice as that same in memory representation of the grammar can be used for error messages and fault tolerance as well as deciding which path to take. Let's finish implementing the whole SelectParser: const { Parser } = require(&quot;chevrotain&quot;) const allTokens = [WhiteSpace, Select, From, Where, Comma, Identifier, Integer, GreaterThan, LessThan] class SelectParser extends Parser { constructor(input) { super(input, allTokens) const $ = this $.RULE(&quot;selectStatement&quot;, () =&gt; { $.SUBRULE($.selectClause) $.SUBRULE($.fromClause) $.OPTION(() =&gt; { $.SUBRULE($.whereClause) }) }) $.RULE(&quot;selectClause&quot;, () =&gt; { $.CONSUME(Select) $.AT_LEAST_ONE_SEP({ SEP: Comma, DEF: () =&gt; { $.CONSUME(Identifier) }}) }) $.RULE(&quot;fromClause&quot;, () =&gt; { $.CONSUME(From) $.CONSUME(Identifier) }) $.RULE(&quot;whereClause&quot;, () =&gt; { $.CONSUME(Where) $.SUBRULE($.expression) }) $.RULE(&quot;expression&quot;, () =&gt; { $.SUBRULE($.atomicExpression) $.SUBRULE($.relationalOperator) $.SUBRULE2($.atomicExpression) \/\/ note the '2' suffix to distinguish \/\/ from the 'SUBRULE(atomicExpression)' \/\/ 2 lines above. }) $.RULE(&quot;atomicExpression&quot;, () =&gt; { $.OR([ {ALT: () =&gt; $.CONSUME(Integer)}, {ALT: () =&gt; $.CONSUME(Identifier)} ]); }) $.RULE(&quot;relationalOperator&quot;, () =&gt; { $.OR([ {ALT: () =&gt; $.CONSUME(GreaterThan)}, {ALT: () =&gt; $.CONSUME(LessThan)} ]) }) Parser.performSelfAnalysis(this) } } Note that as a consequence of the parser having to 'know' its position in the grammar during runtime, the Parsing DSL methods need to be distinguishable when appearing in the same rule. Thus in the &quot;expression&quot; rule above, the second appearance of SUBRULE with atomicExpression parameter has a '2' suffix: $.SUBRULE2($.atomicExpression) Such errors will be detected during self analysis, and will prevent the creation of parser instances with a descriptive error message (fail fast...). But how do we actually use this Parser? \/\/ ONLY ONCE const parser = new SelectParser([]); function parseInput(text) { const lexingResult = SelectLexer.tokenize(text) \/\/ &quot;input&quot; is a setter which will reset the parser's state. parser.input = lexingResult.tokens parser.selectStatement() if (parser.errors.length &gt; 0) { throw new Error(&quot;sad sad panda, Parsing errors detected&quot;) } } const inputText = &quot;SELECT column1 FROM table2&quot; parseInput(inputText) Note that any of the grammar rules can be invoked as the starting rule. There is no 'special' top level entry rule. What is Next? Run &amp; Debug the source code of this tutorial step. Next step in the tutorial: Step 3 - Grammar Actions.","tags":"","url":"Tutorial\/step2_parsing.html"},{"title":"step3 adding actions root","text":"Previous tutorial step - Step 2 - Parsing Tutorial Step 3 - Adding Actions to the Parser. Introduction - The Problem: In the previous tutorial step we have implemented a parser for a &quot;mini&quot; SQL Select grammar. The current problem is that our parser only validates that the input conforms to the grammar. In most real world use cases the parser will also have to output some result\/data structure\/value. The Solutions: Chevrotain supports two very different solutions to this problem: Separation of grammar and user actions (Semantics) using a CST Visitor. Example Embedding user actions (Semantics) inside the grammar rules. Example Before we continue one of the two approaches must be chosen. The main difference between the two approaches is in regards to the question: Where are the user actions (a.k.a semantics) written? When using a CST Visitor the semantics are completely separated from the grammar They could actually be implemented in a different file. This has great benefits for the parser's ease of maintenance and its re-usability. Thus using a CST Visitor is the recommended approach. That is not to say there are no use cases in which embedded actions are better. The main advantage of embedded actions is their performance. Embedded actions are about 50% faster than using a CST Visitor. This may sound like an unbeatable advantage but that is not the case: Chevrotain is so fast that even with that performance penalty. penalty it would beat most other parsing solutions and still be close to the performance of an hand built parser. Tested on Modern V8 The Parsing step is normally just one step of a larger flow, a large performance penalty in one step does not equate to a large performance penalty in the whole flow... Summary It is recommended to use a CST Visitor to separate the semantics(actions) from the syntax(grammar). Prefer embedding the semantics (actions) in the grammar only in use cases where performance is of utmost concern. What is Next? Next step in the tutorial: Step 3a - Separated Actions with a Visitor. Next step in the tutorial: Step 3b - Embedded Actions.","tags":"","url":"Tutorial\/step3_adding_actions_root.html"},{"title":"step3a adding actions visitor","text":"Previous tutorial step - Step 2 - Parsing Tutorial Step 3 - Adding Actions using a CST Visitor. ---&gt; Source Code for this step &lt;--- On code samples: The tutorial uses ES2015+ syntax. See examples of using Chevrotain in other implementation languages. Introduction: In the previous tutorial step we have implemented a parser for a &quot;mini&quot; SQL Select grammar. The current problem is that our parser only validates the input conforms to the grammar, in other words it is just a recognizer. But in most real world use cases the parser will also have to output some result\/data structure\/value. This can be accomplished using a CST (Concrete Syntax Tree) Visitor defined outside our grammar: See in depth documentation of Chevrotain's CST capabilities Enabling CST output in our parser. First we need to enable CST (Concrete Syntax Tree) creation by our parser. This is easily done by passing the &quot;outputCst&quot; parser options object in the super constructor. class SelectParser extends chevrotain.Parser { constructor(input) { \/\/ The &quot;outputCst&quot; flag will cause the parser to create a CST structure on rule invocation super(input, allTokens, {outputCst: true}) \/* rule definitions... *\/ Parser.performSelfAnalysis(this) } } Note that this is the only change needed in the parser. The invocation of any grammar rule will now automatically create a CST. function parseInput(text) { const lexingResult = SelectLexer.tokenize(text) const parser = new SelectParser(lexingResult.tokens) \/\/ CST automatically created. const cstOutput = parser.selectStatement() } Creating a CST Visitor Each Chevrotain parser instance exposes two BaseVisitor classes which can be extended to create custom user visitors. \/\/ BaseVisitor constructors are accessed via a parser instance. const parserInstance = new SelectParser([]); const BaseSQLVisitor = parserInstance.getBaseCstVisitorConstructor() \/\/ This BaseVisitor include default visit methods that simply traverse the CST. const BaseSQLVisitorWithDefaults = parserInstance.getBaseCstVisitorConstructorWithDefaults() class myCustomVisitor extends BaseSQLVisitor { constructor() { super() \/\/ The &quot;validateVisitor&quot; method is a helper utility which performs static analysis \/\/ to detect missing or redundant visitor methods this.validateVisitor() } \/* Visit methods go here *\/ } class myCustomVisitorWithDefaults extends BaseSQLVisitorWithDefaults { constructor() { super() this.validateVisitor() } \/* Visit methods go here *\/ } const myVisitorInstance = new myCustomVisitor() const myVisitorInstanceWithDefaults = new myCustomVisitorWithDefaults() In our example we will use the BaseVisitor constructor (without defaults ) Adding some visitor methods So we now know how to create a CSt visitor. But how do we actually make it perform the actions (semantics) we wish? For that we must create a visit method for each grammar rule. Recall the selectClause grammar from the previous step: selectClause: &quot;SELECT&quot; Identifier (&quot;,&quot; Identifier)*; Lets create a visitor method for the selectClause rule. class SQLToAstVisitor extends BaseSQLVisitor { constructor() { super() this.validateVisitor() } \/\/ The Ctx argument is the current CSTNode's children. selectClause(ctx) { \/\/ Each Terminal or Non-Terminal in a grammar rule are collected into \/\/ an array with the same name(key) in the ctx object. let columns = ctx.Identifier.map((identToken) =&gt; identToken.image) return { type : &quot;SELECT_CLAUSE&quot;, columns : columns } } } So far pretty simple, now lets add another visit method for &quot;selectStatement&quot;. First lets recall it's grammar. selectStatement : selectClause fromClause (whereClause)? And now to the code: class SQLToAstVisitor extends BaseSQLVisitor { constructor() { super() this.validateVisitor() } \/\/ The Ctx argument is the current CSTNode's children. selectClause(ctx) { \/* as above... *\/ } selectStatement(ctx) { \/\/ &quot;this.visit&quot; can be used to visit none-terminals and will invoke the correct visit method for the CstNode passed. let select = this.visit(ctx.selectClause) \/\/ &quot;this.visit&quot; can work on either a CstNode or an Array of CstNodes. \/\/ If an array is passed (ctx.fromClause is an array) it is equivalent \/\/ to passing the first element of that array let from = this.visit(ctx.fromClause) \/\/ &quot;whereClause&quot; is optional, &quot;this.visit&quot; will ignore empty arrays (optional) let where = this.visit(ctx.whereClause) return { type : &quot;SELECT_STMT&quot;, selectClause : select, fromClause : from, whereClause : where } } } We still have a few grammar rules we need to build visitors for. fromClause : &quot;FROM&quot; Identifier whereClause : &quot;WHERE&quot; expression expression : atomicExpression relationalOperator atomicExpression atomicExpression : Integer | Identifier relationalOperator : &quot;&gt;&quot; | &quot;&lt;&quot; lets implement those as well. class SQLToAstVisitor extends BaseSQLVisitor { constructor() { super() this.validateVisitor() } selectStatement(ctx) { \/* as above... *\/ } selectClause(ctx) { \/* as above... *\/ } fromClause(ctx) { const tableName = ctx.Identifier[0].image return { type : &quot;FROM_CLAUSE&quot;, table : tableName } } whereClause(ctx) { const condition = this.visit(ctx.expression) return { type: &quot;WHERE_CLAUSE&quot;, condition: condition } } expression(ctx) { const lhs = this.visit(ctx.atomicExpression[0]) \/\/ The second [1] atomicExpression is the right hand side const rhs = this.visit(ctx.atomicExpression[1]) const operator = this.visit(ctx.relationalOperator) return { type: &quot;EXPRESSION&quot;, lhs: lhs, operator: operator, rhs: rhs } } \/\/ these two visitor methods will return a string. atomicExpression(ctx) { if (ctx.Integer[0]) { return ctx.Integer[0].image } else { return ctx.Identifier[0].image } } relationalOperator(ctx) { if (ctx.GreaterThan[0]) { return ctx.GreaterThan[0].image } else { return ctx.LessThan[0].image } } } Gluing it all together So we know how to create a CST Visitor, but how do we actually use it? \/\/ A new parser instance with CST output enabled. const parserInstance = new SelectParser([], {outputCst: true}) \/\/ Our visitor has no state, so a single instance is sufficient. const toAstVisitorInstance = new SQLToAstVisitor() function toAst(inputText) { \/\/ Lex const lexResult = selectLexer.tokenize(inputText) parserInstance.input = lexResult.tokens \/\/ Automatic CST created when parsing const cst = parserInstance.selectStatement() if (parserInstance.errors.length &gt; 0) { throw Error(&quot;Sad sad panda, parsing errors detected!\\n&quot; + parserInstance.errors[0].message) } \/\/ Visit const ast = toAstVisitorInstance.visit(cst) return ast } What is Next? Run &amp; Debug the source code of this tutorial step. Next step in the tutorial: Step 4 - Fault Tolerance.","tags":"","url":"Tutorial\/step3a_adding_actions_visitor.html"},{"title":"step3b adding actions embedded","text":"Previous tutorial step - Step 2 - Parsing Tutorial Step 3 - Adding Embedded Actions to the Parser. ---&gt; Source Code for this step &lt;--- On code samples: The tutorial uses ES2015+ syntax. See examples of using Chevrotain in other implementation languages. Introduction: In the previous tutorial step we have implemented a parser for a &quot;mini&quot; SQL Select grammar. The current problem is that our parser only validates the input conforms to the grammar. In most real world use cases the parser will also have to output some result\/data structure\/value. This can be accomplished using two features of the Parsing DSL: CONSUME will return The IToken object consumed. SUBRULE will return the result of the grammar rule invoked. A simple contrived example: $.RULE(&quot;topRule&quot;, () =&gt; { let result = 0 $.MANY(() =&gt; { $.OR([ {ALT: () =&gt; { result += $.SUBRULE($.decimalRule)}}, {ALT: () =&gt; { result += $.SUBRULE($.IntegerRule)}} ]) }) return result }) $.RULE(&quot;decimalRule&quot;, () =&gt; { const decimalToken = $.CONSUME(Decimal) return parseFloat(decimalToken.image) }) $.RULE(&quot;IntegerRule&quot;, () =&gt; { const intToken = $.CONSUME(Integer) return parseInt(intToken.image) }) The decimalRule and IntegerRule both return a javascript number (using parseInt\/parseFloat). and the topRule adds it to the final result. Back To the mini SQL Select grammar: For this grammar lets build a more complex data structure (an AST) instead of simply returning a number. Our selectStatement rule will now return an object with four properties: $.RULE(&quot;selectStatement&quot;, () =&gt; { let select, from, where select = $.SUBRULE($.selectClause) from = $.SUBRULE($.fromClause) $.OPTION(() =&gt; { where = $.SUBRULE($.whereClause) }) return { type : &quot;SELECT_STMT&quot;, selectClause : select, fromClause : from, \/\/ may be undefined if the OPTION was not entered. whereClause : where } }) Three of those properties (selectClause \/ fromClause \/ whereClause) are the results of invoking other parser rules. Lets look at the &quot;selectClause&quot; rule implemntaiton: $.RULE(&quot;selectClause&quot;, () =&gt; { let columns = [] $.CONSUME(Select); $.AT_LEAST_ONE_SEP({SEP:Comma, DEF:() =&gt; { \/\/ accessing a token's string via getImage utility columns.push($.CONSUME(Identifier).image) }}) return { type : &quot;SELECT_CLAUSE&quot;, columns : columns } }) In the selectClause rule we access the image property of the Identifier token returned from CONSUME and push each of these strings to the columns array. What is Next? Run &amp; Debug the source code of this tutorial step. Next step in the tutorial: Step 4 - Fault Tolerance.","tags":"","url":"Tutorial\/step3b_adding_actions_embedded.html"},{"title":"step4 fault tolerance","text":"Previous tutorial steps Step 3a - Separated Actions. Step 3b - Embedded Actions. Tutorial Step 4 - Fault tolerance and Error recovery. ---&gt; Source Code for this step &lt;--- Introduction: In the previous tutorial steps we have learned how to build a parser for a simple grammar. Our parser can handle valid inputs just fine, but what happens if the input is not perfectly valid? For example when building an editor for a programing language, the input is often not completely valid, yet the editor is still expected to provide functionality (outline\/auto-complete\/navigation\/error locations...) even for invalid inputs. Chevrotain uses several fault tolerance \/ error recovery heuristics, which generally follow error recovery heuristics used in Antlr3. In Rule Single Token insertion: Happens when: A token Y is expected. But a token X is found. X is a valid token after the missing Y token. A Y token will be automatically inserted into the token stream. For example: in a JSON text colons are used between keys and values. \/\/ GOOD { &quot;key&quot; : 666} \/\/ BAD, missing colon { &quot;key&quot; 666} If we try parsing the &quot;bad&quot; example, after consuming: { &quot;key&quot; We expect a colon token (Y). We will find a number(666) token (X). After the colon token, a number token is valid. Therefore the missing colon will be automatically &quot;inserted&quot;. This heuristic's behavior can be customized by the following methods: canTokenTypeBeInsertedInRecovery getTokenToInsert In Rule Single Token deletion: Happens when: A token Y is expected. But a token X is found. And immediately after X an Y is found. The unexpected token X will be skipped (deleted) and the parsing will continue. For example: lets look at the case of a \/\/ GOOD { &quot;key&quot; : 666} \/\/ BAD, redundant &quot;}&quot; { &quot;key&quot; }: 666} If we try parsing the &quot;bad&quot; example, after consuming: { &quot;key&quot; We are expecting a colon token (Y). But we found right brackets (X) instead. The next token (&quot;:&quot;) is a colon token (Y) which the one we originally expected. Therefore the redundant right brackets &quot;}&quot; will be skipped (deleted) and the parser will consume the number token. The following re-sync recovery examples use this sample json like grammar: object : &quot;{&quot; objectItem (comma objectItem)* &quot;}&quot; objectItem : stringLiteral &quot;:&quot; value value : object | stringLiteral | number | ... In Rule Repetition Re-Sync recovery: Repetition re-sync recovery happens when: The parser is in a repetition(MANY\/AT_LEAST_ONE\/MANY_SEP\/AT_LEAST_ONE_SEP). The parser has consumed the last iteration and is about to &quot;exit&quot; the repetition. The next token X is invalid right after the repetition ended. In such a situation the parser will attempt to skip tokens until it detects the beginning of a another iteration of the repetition or the token it originally expected after the last iteration. There are a couple of edge cases in which other recovery methods will be preferred: If single token insertion\/deletion can be performed, it is always preferred as it skips fewer tokens. If between rules re-sync recovery can be performed (see below) and it can be done by skipping fewer tokens. Between rules re-sync will be preferred over repetition re-sync recovery. The same principle applies, the heuristics are greedy and &quot;prefer&quot; to skip the fewest number of tokens. Example: { &quot;key1&quot; : 1, &quot;key2&quot; : 2 666 \/\/ '666' should not appear here! &quot;key3 : 3, &quot;key4 : 4 } If we try parsing this input example, after consuming: { &quot;key1&quot; : 1, &quot;key2&quot; : 2 The parser in in a repetition of *(comma objectItem) ** After consuming '&quot;key2&quot; : 2' the parser &quot;thinks&quot; it has consumed the last iteration as the next comma is missing. The next token (X) encountered is &quot;666&quot; which is invalid in that position as the parser expected a &quot;}&quot; after the repetition ends. The parser will throw away the following tokens [666, &quot;key3&quot;, :, 3] and re-sync to the next comma (,) to continue a another iteration. Note that in such a situation some input would be lost, (the third key), however the fourth key will still be parsed successfully! Between Rules Re-Sync recovery: Between Rules re-sync recovery happens when the parser encounters a parser error inside a rule which it cannot recover from in other ways. For example: An unexpected Token as been found (MisMatchTokenException) but single token insertion\/deletion cannot resolve it. None of the alternatives in an OR match. A Repetition of AT_LEAST_ONE cannot match even one iteration. ... In re-sync recovery the parser will skip tokens from the token stream until it detects a point it can continue parsing from. The parser will try to skip as few tokens as possible and re-sync to the closest rule in the rule stack. An Abstract example: Grammar Rule A called Grammar Rule B which called Grammar Rule C (A -&gt; B -&gt; C). In Grammar Rule C a parsing error happened which we can not recover from. The Parser will now skip tokens until it find a token that can appear immediately after either: The call of C in B The call of B in A A concrete example: For the following invalid json input: { &quot;firstName&quot;: &quot;John&quot;, &quot;someData&quot;: { &quot;bad&quot; :: &quot;part&quot; }, \/\/ &lt;-- too many colons in the nested object &quot;isAlive&quot;: true, &quot;age&quot;: 25 } When encountering the the redundant colon the rule stack will be as follows: object --&gt; top level object objectItem --&gt; &quot;someData&quot;: ... - second item in the top level object value --&gt; { &quot;bad&quot; :: &quot;part&quot; } - the value of the &quot;someData&quot; key object --&gt; { &quot;bad&quot; :: &quot;part&quot; } - the value of the &quot;someData&quot; key objectItem --&gt; &quot;bad&quot; :: &quot;part&quot; - the single item in the inner object. value --&gt; : &quot;part&quot; - the value with the colon prefix The redundant colon will cause an error (NoViableAltException) as the value rule will not be able to decide which alternative to take as none would match. This means the parser needs to find a token to synchronize to, lets check the options: After value called by ObjectItem --&gt; none After objectItem called by object --&gt; comma. After object called by value --&gt; none. After value called by ObjectItem --&gt; none after objectItem called by object --&gt; comma (again). so the Parser will re-sync to the closest ObjectItem if it finds a comma in the remaining token stream. Therefore the following tokens will be skipped: [':', '&quot;part&quot;', '}'] And the Parser continue from the &quot;nearest&quot; objectItem rule as if it was successfully invoked. Thus the next two items will appear be parsed successfully even though they were preceded by a syntax error! Enabling All Recovery mechanisms By default fault tolerance and error recovery heuristics are disabled. They can be enabled by passing a optional recoveryEnabled parameter (default true) To the parser's constructor constructor. CST output for re-synced rules: When using Concrete Syntax Tree output A re-synced will return a CSTNode with the boolean &quot;recoveredNode&quot; flag marked as true. Additionally a recovered node may not have all its contents (children dictionary) filled as only the Terminals and None-Terminals encountered before the error which triggered the re-sync will be present. This means that code that handles the CST (CST Walker or Visitor) must not assume certain content is always present on a CstNode. Instead it must be very defensive to avoid runtime errors. Embedded Actions (semantics) and the return values of re-synced rules: Just being able to continue parsing is not enough, as &quot;someone&quot; probably expects a returned value from the sub-rule we have recovered from. By default undefined will be returned from a recovered rule, however this should most likely be customize in any but the most simple cases. Customization is done during the definition of the grammar RULE. The third parameter(config) may contain a recoveryValueFunc property which is a function that will be invoked to produce the returned value in case of re-sync recovery. Disabling Re-Sync Recovery per rule.: Re-Sync recovery is enabled by default for all rules. In some cases it may be appropriate to disable re-sync recovery for a specific rule. This is (once again) done during the definition of the grammar RULE. The third parameter(config) may contain a resyncEnabled* property that controls whether or not re-sync is enabled for the rule. Difference between &quot;In-Rule&quot; and &quot;Between Rules&quot; recovery. The main difference is that &quot;In-Rule&quot; recovery fixes the problem in the scope of a single rule, while Between Rules recovery will fail at least one parsing rule (and perhaps many more). Thus the latter tends to &quot;lose&quot; more of the original input and requires additional definitions (what should be returned value of a re-synced rule?). What is Next? Run &amp; Debug the source code of this tutorial step.","tags":"","url":"Tutorial\/step4_fault_tolerance.html"},{"title":"concrete syntax tree","text":"Automatic Concrete Syntax Tree Creation Chevrotain has the capability to automatically create a concrete syntax tree (CST) during parsing. A CST is a simple structure which represents the entire parse tree. It contains information on every token parsed. The main advantage of using the automatic CST creation is that it enables writing &quot;pure&quot; grammars. This means that the semantic actions are not embedded into the grammar implementation but are instead completely separated from it. This separation of concerns makes the grammar easier to maintain and makes it easier to implement different capabilities on the grammar, for example: separate logic for compilation and for IDE support. Differences between an AST and a CST. There are two major differences. An Abstract Syntax Tree would not normally contain all the syntactic information. This mean the exact original text could not be re-constructed from the AST. An Abstract Syntax Tree would not represent the whole syntactic parse tree. It would normally only contain nodes related to specific parse tree nodes, but not all of those (mostly leaf nodes). How to enable CST output? In the future this capability will be enabled by default. Currently this feature must be explicitly enabled by setting the outputCst flag. In the parser configuration object. class MyParser extends chevrotain.Parser { constructor(input) { super(input, allTokens, {outputCst : true}) } } The structure of the CST The structure of the CST is very simple. Run the CST creation example in the online playground. Note that the following examples are not runnable nor contain the full information. These are just snippets to explain the core concepts. export type CstElement = IToken | CstNode export type CstChildrenDictionary = { [elementName:string]:CstElement[] } export interface CstNode { readonly name:string readonly children:CstChildrenDictionary readonly recoveredNode?:boolean } A single CstNode corresponds to a single grammar rule's invocation result. $.RULE(&quot;qualifiedName&quot;, () =&gt; { }) input = &quot;&quot; output = { name: &quot;qualifiedName&quot;, children: {} } Each Terminal will appear in the children dictionary using the terminal's name as the key and an array of IToken as the value. $.RULE(&quot;qualifiedName&quot;, () =&gt; { $.CONSUME(Identifier) $.CONSUME(Dot) $.CONSUME2(Identifier) }) input = &quot;foo.bar&quot; output = { name: &quot;qualifiedName&quot;, children: { Dot : [&quot;.&quot;], Identifier : [&quot;foo&quot;, &quot;bar&quot;] } } Non-Terminals are handled similarly to Terminals except each item in the value's array Is the CstNode of the corresponding Grammar Rule (Non-Terminal). $.RULE(&quot;qualifiedName&quot;, () =&gt; { $.SUBRULE($.singleIdent) }) $.RULE(&quot;singleIdent&quot;, () =&gt; { $.CONSUME(Identifier) }) input = &quot;foo&quot; output = { name: &quot;qualifiedName&quot;, children: { singleIdent : [ { name: &quot;singleIdent&quot;, children: { Identifier : [&quot;foo&quot;] } } ] } } In-Lined Rules So far the CST structure is quite simple, but how would a more complex grammar be handled? $.RULE(&quot;statements&quot;, () =&gt; { $.OR([ \/\/ let x = 5 {ALT: () =&gt; { $.CONSUME(Let) $.CONSUME(Identifer) $.CONSUME(Equals) $.SUBRULE($.expression) }}, \/\/ select age from employee where age = 120 {ALT: () =&gt; { $.CONSUME(Select) $.CONSUME2(Identifer) $.CONSUME(From) $.CONSUME3(Identifer) $.CONSUME(Where) $.SUBRULE($.expression) }} ]) }) Some of the Terminals and Non-Terminals are used in both alternatives. It is possible to check for the existence of distinguishing terminals such as the &quot;Let&quot; and &quot;Select&quot;. But this is not a robust approach. let cstResult = parser.qualifiedName() if (cstResult.children.Let.length &gt; 0) { \/\/ Let statement \/\/ do something... } else if (cstResult.children.Select.length &gt; 0) { \/\/ Select statement \/\/ do something else. } Alternatively it is possible to refactor the grammar in such a way that both alternatives Would be completely wrapped in their own Non-Terminal rules. $.RULE(&quot;statements&quot;, () =&gt; { $.OR([ {ALT: () =&gt; $.SUBRULE($.letStatement)}, {ALT: () =&gt; $.SUBRULE($.selectStatement)} ]) }) This is the recommended approach in this case as more and more alternations are added the grammar rule will become too difficult to understand and maintain due to verbosity. However sometimes refactoring out rules is too much, this is where in-lined rules arrive to the rescue. $.RULE(&quot;statements&quot;, () =&gt; { $.OR([ \/\/ let x = 5 { NAME: &quot;$letStatement&quot;, ALT: () =&gt; { $.CONSUME(Let) $.CONSUME(Identifer) $.CONSUME(Equals) $.SUBRULE($.expression) }}, \/\/ select age from employee where age = 120 { NAME: &quot;$selectStatement&quot;, ALT: () =&gt; { $.CONSUME(Select) $.CONSUME2(Identifer) $.CONSUME(From) $.CONSUME3(Identifer) $.CONSUME(Where) $.SUBRULE($.expression) }} ]) }) output = { name: &quot;statements&quot;, children: { $letStatement : [\/*...*\/], $$selectStatement : [\/*...*\/] } } Providing a NAME property to the DSL methods will create an in-lined rule. It is equivalent to extraction to a separate grammar rule with two differences: To avoid naming conflicts in-lined rules must start with a dollar($) sign. In-lined rules do not posses error recovery (re-sync) capabilities as do regular rules. Syntax Limitation: The NAME property of an in-lined rule must appear as the first property of the DSLMethodOpts object. \/\/ GOOD $.RULE(&quot;field&quot;, () =&gt; { $.OPTION({ NAME:&quot;$modifier&quot;, DEF: () =&gt; { $.CONSUME(Static) } }) }) \/\/ Bad - won't work. $.RULE(&quot;field&quot;, () =&gt; { $.OPTION({ DEF: () =&gt; { $.CONSUME(Static) }, NAME:&quot;$modifier&quot; }) }) CST And Error Recovery CST output is also supported in combination with automatic error recovery. This combination is actually stronger than regular error recovery because even partially formed CstNodes will be present on the CST output and be marked using the recoveredNode&quot; boolean property. For example given this grammar and assuming the parser re-synced after a token mismatch at the &quot;Where&quot; token: $.RULE(&quot;SelectClause&quot;, () =&gt; { $.CONSUME(Select) $.CONSUME2(Identifer) $.CONSUME(From) $.CONSUME3(Identifer) $.CONSUME(Where) $.SUBRULE($.expression) }) \/\/ mismatch token due to typo at &quot;wherrrre&quot;, parsing halts and re-syncs to upper rule so \/\/ the suffix &quot;wherrrre age &gt; 25&quot; is not parsed. input = &quot;select age from persons wherrrre age &gt; 25&quot; output = { name: &quot;SelectClause&quot;, children: { Select: [&quot;select&quot;], Identifier: [&quot;age, persons&quot;], From: [&quot;from&quot;], Where: [\/*nothing here, due to parse error*\/], expression: [\/*nothing here, due to parse error*\/], }, \/\/ This marks a recovered node. recoveredNode: true } This accessibility of partial parsing results means some post-parsing logic may be able to perform farther analysis for example: offer auto-fix suggestions or provide better error messages. Traversing a CST Structure. So we now know how to create a CST and it's internal structure. But how do we traverse this structure and perform semantic actions? Examples for such semantic actions: Creation of an Abstract Syntax Tree (AST) to be later used in the rest of the compilation pipeline. Running the input text in an interpreter, for example a Calculator's grammar and input can be evaluated to a numerical value. Extracting specific pieces of information from the input, I.E data mining. One option would be to &quot;manually&quot; recursively &quot;walk&quot; the output CST structure. export function toAst(cst) { const children = cst.children switch (cst.name) { case &quot;selectStatement&quot;: { let columnsListCst = children.columnsList[0] let fromClauseCst = children.fromClause[0] let columnsListAst = toAst(columnsListCst) let fromClauseAst = toAst(fromClauseCst) return { type: &quot;SelectStatementAst&quot;, columns: columnsListAst, from: fromClauseAst } } case &quot;columnsList&quot;: { let columnName = children.identifier[0].image \/*...*\/ } case &quot;fromClause&quot;: { \/*...*\/ } default: { throw new Error(`CST case handler not implemented for CST node &lt;${cst.name}&gt;`) } } } This is a valid approach, however it can be somewhat error prone: No validation that the case names match the real names of the CST Nodes. The validation for missing case handler (default case) depends on attempting to run toAst with invalid input. (Fail slow instead of fail fast...) In-Lined Rules may cause ambiguities as they should be matched on the fullName property not the name property. A more robust alternative. For the impatient, See a full runnable example: Calculator Grammar with CSTVisitor interpreter Chevrotain provides a CSTVisitor class which can make traversing the CST less error prone. \/\/ The base Visitor Class can be accessed via a Parser **instance**. const BaseCstVisitor = myParserInstance.getBaseCstVisitorConstructor() class SqlToAstVisitor extends BaseCstVisitor { constructor() { super() \/\/ This helper will detect any missing or redundant methods on this visitor this.validateVisitor() } selectStatement(ctx) { \/\/ ctx.columnsList is an array, while this.visit accepts a CSTNode \/\/ but if an array is passed to this.visit it will act as though the first element of the array has been passed. \/\/ this means &quot;this.visit(ctx.columnsList)&quot; is equivalent to &quot;this.visit(ctx.columnsList[0])&quot; let columnsListAst = this.visit(ctx.columnsList) let fromClauseAst = this.visit(ctx.fromClause) return { type: &quot;SelectStatementAst&quot;, columns: columnsListAst, from: fromClauseAst } } columnsList(ctx) { let columnName = ctx.identifier[0].image \/*...*\/ } \/\/ Optional &quot;IN&quot; argument fromClause(ctx, inArg) { \/*...*\/ } \/\/ Visitor methods for in-lined rules are created by appending the in-lined rule name to the parent rule name. fromClause$INLINED_NAME(ctx) { \/*...*\/ } } Each visitor method will be invoked with the respective CSTNode's children as the first argument (called ctx in the above example). Recursively visiting None-Terminals can be accomplished by using the this.visit method. It will invoke the appropriate visit method for the CSTNode argument. The this.visit method can also be invoked on an array on CSTNodes in that case It is equivalent to calling it on the first element of the input array. Each visit method can return a value which can be used to combine the traversal results. The this.validateVisitor() method can be used to detect missing or redundant visitor methods. For example due to a refactoring of the grammar or a typo. Visitor methods support an optional &quot;IN&quot; parameter. Do we always have to implement all the visit methods? No, sometimes we only need to handle a few specific CST Nodes In that case use getBaseCstVisitorConstructorWithDefaults() to get the base visitor constructor. This base visitor includes a default implementation for all visit methods which simply invokes this.visit on all none terminals in the CSTNode's children. \/\/ The base Visitor Class can be accessed via a Parser **instance**. const BaseCstVisitorWithDefaults = myParserInstance.getBaseCstVisitorConstructorWithDefaults() class SqlColumnNamesVisitor extends BaseCstVisitorWithDefaults { constructor() { super() this.result = [] this.validateVisitor() } fromClause(ctx) { \/\/ collect only the names of the columns this.result.push(ctx.Identifier[0].image) } \/\/ All other visit methods will be &quot;filled&quot; automatically with the default implementation. } Note that when using a visitor with default visit implementations It is not possible to return values from the visit methods because the default implementation does not return any value, only traverses the CST thus the chain of returned values will be broken. Performance of CST building. On V8 (Chrome\/Node) building the CST was measured at about 65% of the performance versus a pure grammar's runtime. This is substantial but considering Chevrotain is already very fast and that parsing is usually just one part of a larger flow, than unless there a special edge case which requires maximum performance than the benefits of using the CST (modularity \/ ease of maintenance) by far outweigh the costs (reduced performance). Online CST Benchmark","tags":"","url":"Deep_Dive\/concrete_syntax_tree.html"},{"title":"custom apis","text":"Building Custom Parsing APIs Chevrotain can be used as the underlying engine for other parsing libraries. The general flow is: Creating a GAST (Grammar AST) data structure. Resolving and validating the GAST Structure. Generating the parser code and executing. Can be done &quot;in memory&quot; without writing to file in certain runtime envs. TLDR Skip to runnable examples The GAST structure This structure is made up of the following classes: Rule Terminal NonTerminal Alternation Option Repetition RepetitionWithSeparator RepetitionMandatory RepetitionMandatoryWithSeparator Flat (sequence) For example to define a grammar rule for a fully qualified name: fqn : Ident (Dot Ident) Is equivalent to: const {createToken, Rule, Terminal, Repetition} = require(&quot;chevrotain&quot;) const Ident = createToken({name:&quot;Ident&quot;, pattern:\/[a-zA-Z]\\w+\/}) const Dot = createToken({name:&quot;Dot&quot;, pattern:\/\\.\/}) const fqn = new Rule({name:&quot;fqn&quot;, definition:[ new Terminal({terminalType:Ident}), new Repetition({definition:[ new Terminal({terminalType:Dot}), new Terminal({terminalType:Ident}) ]}) ]}) Important to note that: By default the definition array for each GAST class acts as a sequence, However in the case of Alternation each element in the definition array represents a different alternative which should be wrapped in a Flat class. e.g: const {Flat, Alternation} = require(&quot;chevrotain&quot;) const gastAlts = new Alternation({definition:[ \/\/ first alternative new Flat({definition:[\/*...*\/]}), \/\/ second alternative new Flat({definition:[\/*...*\/]}), \/\/ third alternative new Flat({definition:[\/*...*\/]}) ]}) Resolving and Validating the GAST structure Chevrotain exposes three functions for this purpose: resolveGrammar NonTerminals are often referenced by their name as cyclic references will make a direct object reference impossible, for example with right recursion: rule1: A (rule1)? The resolveGrammar function will resolve (mutate the input rules) such &quot;name only&quot; references to the actual Rule instance. For any reference that cannot be resolved an error object will be outputted, this object will contain an error message which can be customized by providing a custom implementation of IGrammarResolverErrorMessageProvider. The default implementation also exported as part of the public API. validateGrammar Chevrotain expose a set of checks on the grammar structure that it is highly recommended to execute. These checks will detect ambiguous alternatives, left recursion, conflicting Terminals &amp; NonTerminal names and more... validateGrammar is side effect free and like resolveGrammar will return an array of error objects. The error messages in these objects can once again be customized by providing a IGrammarResolverErrorMessageProvider optionally based on the default implementation assignOccurrenceIndices Chevrotain has certain constraint on the &quot;shape&quot; of the generated code. The relevant one in this case is the unique numerical suffixes for the DSL methods. The assignOccurrenceIndices function will take care of this requirement by mutating the idx property on the GAST classes accordingly. A Note on Custom Error Messages As mentioned earlier validateGrammar assignOccurrenceIndices optionally accept a errMsgProvider option. Note that all custom error message builders receive a GAST instance as input. What this means is that the GAST classes created by the implementor of the custom API can be augmented with additional information to enable producing better error messages. e.g: A Parser Generator style API using an EBNF notation in a separate file. may add location (line\/column\/offset) information to be able to link to locations in the original EBNF styled file. A Parser combinator API may attempt to reconstruct the original text of its API invocations to give better hints to assist in locating the original error. Code Generation &amp; Execution There are two APIs for code generation and execution. generateParserModule This will generate the string literal of a UMD module. This UMD pattern is consumable in all standard JS runtimes &amp; module implementations. The approach is to generate the string literal and write it to a file for later consumption. However for development purposes or under certain runtimes it is possible to skip the file creation For example see the require-from-string generateParserFactory This API skips string literal and directly evals (new Function(...)) the code and returns a factory that can be used to create Parser instances. This can be useful for development and testing purposes but be wary as certain execution environments disallow the use of eval\/new Function. Specifically webpages with content security policy enabled and browser extensions. However if the custom API is targeting only a node.js runtime this can be very useful for example for a combinator style API in which code generation is best avoided. Runnable Examples Combinator Style Generator Style (TBD) Limitations The Following features are currently unsupported. Embedded actions, which means the only way to get output from the parser is by enabling automatic Concrete Syntax Tree creation. Gates\/Predicates. Parameterized Rules.","tags":"","url":"Deep_Dive\/custom_apis.html"},{"title":"custom token patterns","text":"Custom Token Patterns See: Runnable example for quick starting. Background Normally a Token's pattern is defined using a JavaScript regular expression: let IntegerToken = createToken({name: &quot;IntegerToken&quot;, pattern: \/\\d+\/}) However in some circumstances the capability to provide a custom pattern matching implementation may be required. There are a few use cases in which a custom pattern could be used: The token cannot be easily (or at all) defined using pure regular expressions. When context on previously lexed tokens is needed. For example: Lexing Python like indentation using Chevrotain. Workaround performance issues in specific regExp engines by providing a none regExp matcher implementation: WebKit\/Safari multiple orders of magnitude performance degradation for specific regExp patterns \u00f0\u009f\u0098\u009e Usage A custom pattern has a similar API to the API of the RegExp.prototype.exec function. But with a small constraint. A custom pattern should behave as though the RegExp sticky flag has been set. This means that attempted matches must begin at the offset argument, not at the start of the input. The basic syntax for supplying a custom pattern is defined by the ICustomPattern interface. Example: function matchInteger(text, startOffset) { let endOffset = startOffset let charCode = text.charCodeAt(endOffset) \/\/ 0-9 digits while (charCode &gt;= 48 &amp;&amp; charCode &lt;= 57) { endOffset++ charCode = text.charCodeAt(endOffset) } \/\/ No match, must return null to conform with the RegExp.prototype.exec signature if (endOffset === startOffset) { return null } else { let matchedString = text.substring(startOffset, endOffset) \/\/ according to the RegExp.prototype.exec API the first item in the returned array must be the whole matched string. return [matchedString] } } const IntegerToken = createToken({name: &quot;IntegerToken&quot;, pattern: { exec: matchInteger }}) Using an Object literal with only a single property is still a little verbose so an even more concise syntax is also supported: const IntegerToken = createToken({ name: &quot;IntegerToken&quot;, pattern: matchInteger }) Using Previous Lexing Context A custom token matcher has two optional arguments which allows accessing the current result of the tokenizer. Lets expand the previous example to only allow lexing integers if the previous token was not an identifier (contrived example). const { tokenMatcher } = require(&quot;chevrotain&quot;) function matchInteger(text, offset, matchedTokens, groups) { let lastMatchedToken = _.last(matchedTokens) \/\/ An Integer may not follow an Identifier if (tokenMatcher(lastMatchedToken, Identifier)) { \/\/ No match, must return null to conform with the RegExp.prototype.exec signature return null } \/\/ rest of the code from the example above... } A larger and non contrived example can seen here: Lexing Python like indentation using Chevrotain. It is important to note that The matchedTokens and groups arguments match the token and groups properties of the tokenize output (ILexingResult). These arguments are the current state of the lexing result so even if the lexer has performed error recovery any tokens found in those arguments are still guaranteed to be in the final result.","tags":"","url":"Deep_Dive\/custom_token_patterns.html"},{"title":"syntactic content assist","text":"Syntactic Content Assist See: Runnable example for quick starting. Detailed Docs: Chevrotain provides Syntactic Content assist Capabilities. These can be accessed via the computeContentAssist method. Note that this feature only provides syntactic suggestions (meaning next possible token types) not semantic suggestions. It could be used as a building block in a semantic suggestions provider, but it cannot do this on &quot;its own&quot;. Also note that this feature is implemented as a separate interpreted backtracking parser, completely unrelated to the normal parsing flow except for using the same internal grammar representation. This causes several important limitations: Performance: The Content assist feature is about x10 times slower than the normal parsing flow. No Embedded actions will be performed. Error Recovery is unsupported Gates \/ Predicates are unsupported. These limitations may seem daunting at first, but should not cause great problems in actual practice. The following sections will discuss each limitation in details. Slow Performance. An order of magnitude slower performance may at first sound like a horrible thing. Lets put this in perspective for relevant use cases: Being an order of magnitude slower also means approximately the same speed as Jison. Tested on Chrome 54, See: performance benchmark. Smaller input Size 1: Content Assist is requested for an offset inside a text, this means that on average only half the text input will have to be parsed. Suddenly the problem is halved... Smaller input size 2: Syntactic Content Assist is requested for a single text input(file), while standard parsing flow may need to handle dozens or hundreds of inputs at once. Less work: The ~x10 performance difference was measured when comparing pure grammars without any embedded actions. In a real world scenario the difference will be smaller as the base parser will have embedded actions with their own runtime cost while the content assist parser will always remain a pure grammar. Smart beats fast: Content Assist is normally used in a code editor. A code editor should be by definition incremental as it does not matter how smart the error recovery is or how fast the parser, re-parsing a whole file for every single changed character will simply not scale, both from a performance perspective and from the requirement of handling partially invalid inputs. What this means is that if a code editor is already (as it should be) incremental. It could invoke the call to computeContentAssist on a subset of the input as well. This subset could easily be several orders of magnitudes smaller, thus all performance concerns are resolved. Example: Imagine a 1,000 lines JavaScript file where a single 10 lines function is being edited and content assist is requested inside that small function. ```javascript \/\/ line 1 \/\/ . \/\/ . \/\/ . \/\/ line 600 function foo() { \/\/ content assist requested somewhere in this function } \/\/ line 610 \/\/ . \/\/ . \/\/ . \/\/ line 1000 ``` There is no need no re-parse the whole file, Instead only the text of that function should be sent To computeContentAssist and the &quot;startRule&quot; should be &quot;functionDeclaration&quot;. Therefore only ~10 lines of text will have to be re-parsed to provide syntactic content assist. This is a 1\/100 difference in input size which is two orders of magnitude smaller. No support for Embedded Actions and Error Recovery. Once again, Content assist is often used in a code Editor's context. Embedded actions will normally only output useful results for valid parts of the input. Error recovery can help with invalid inputs by performing small automatic fixes to the input and more often by completely skipping (re-syncing) parts of the input until a &quot;valid&quot; section is encountered. The problem is that usually the code area where content assist is requested is also currently being heavily modified by the user and is unlikely to able to be successfully parsed or automatically fixed, instead it will probably be skipped (re-synced) entirely. What this means is that for input areas that are currently being edited (or even written from scratch) by the user Embedded actions and error recovery are less useful anyhow. And if as described in the previous section an incremental approach to using the content assist will also resolve the issue in which the content assist position follows a syntax error. Example: let three = \/\/ syntax error, missing expression let five = 5 let six = 1 + \/\/ &lt;-- content assist requested after the '+' return Math.max(five, six) The first and third statements are syntactically invalid. Error recovery is likely to re-sync to the following statements instead of resolving this with single token insertion \/ deletion. Therefore the results of embedded actions on these statements will not be useful. Content assist is request in the third statement after the &quot;+&quot; operator. If we try to send the whole text to request content assist suggestions until the offset after the &quot;+&quot; operator No suggestions will be found due to the syntax error on the first statement. However if we only send the text of the third statement (&quot;let six = 1 + &quot;) content assist will work successfully. No support for Gates \/ Predicates. Gates \/ Predicates act is limiters of the available grammar. These constructs are not often used in grammars which reduces the severity of this limitation. But more importantly this does not mean that grammars using Gates cannot use the content assist functionality. Rather that in some pathological edge cases the suggestions may not be completely valid. In details: the content assist parser is a &quot;backtracking&quot; Parser, which means it will try all the paths until it finds a valid one. It disregards the max token lookahead constraints of the base Chevrotain parser. Therefor for a grammar path that has been disabled by a Gate \/ Predicate to be offered as content assist suggestion: Not only does the fixed tokens lookahead have to match at the Gate's position. The whole input following the Gate's position must also match. This is a much stronger condition (K tokens vs dozens\/hundreds\/infinity) thus making this issue a very minor one.","tags":"","url":"Deep_Dive\/syntactic_content_assist.html"},{"title":"generating syntax diagrams","text":"Generating Syntax Diagrams for a grammar. It is often useful to visually inspect a grammar's syntax diagrams during development or for documentation purposes. This document contains instructions on how to generate Syntax railroad diagrams for a Chevrotain grammar using the railroad-diagrams library by @tabatkins. Examples: JSON Syntax diagrams. CSS Syntax diagrams. Features: Highlight usages and definitions on mouse hover. Scroll to definition of non-terminal on mouse click. Instructions: Chevrotain provides an the createSyntaxDiagramsCode API to generate the html source code of syntax diagrams. This html source code can then be used by an end user in either node.js or a browser: By writing it directly to the disk in a pure node.js runtime scenario. By inserting it dynamically into an iframe in a browser scenario. Examples: Generating syntax diagrams to disk Self contained, no need for Chevrotain or the grammar when running the html. Generating syntax diagrams dynamically into an iframe Requires loading both Chevrotain and the grammar when running the html. Advance Custom Usage The logic for generating the HTML is quite trivial and the generated code itself is also very simple with a decent separation of concerns. These can be used as a basis for creating more advanced custom scenarios, for example: Adding a module loader such as system.js\/require.js Dynamically rendering diagrams of a Grammar in an IDE. Rendering diagrams of a pure EBNF grammar (Not a Chevrotain grammar) as the diagrams are rendered using a serialized format.","tags":"","url":"Building_Grammars\/generating_syntax_diagrams.html"},{"title":"resolving grammar errors","text":"Resolving Grammar Errors Common Prefix Ambiguities. Common Prefix Ambiguities. Imagine the following grammar: myRule: &quot;A&quot; &quot;B&quot; | &quot;A&quot; &quot;B&quot; &quot;C&quot; The first alternative is a prefix of the second alternative. Now lets consider the input [&quot;A&quot;, &quot;B&quot;]. For this input the first alternative would be matched as expected. However for the input [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;] the first alternative would still be matched but this time incorrectly as alternation matches are attempted in order. There are two ways to resolve this: Reorder the alternatives so that shorter common prefix lookahead paths appears after the longer ones. myRule: &quot;A&quot; &quot;B&quot; &quot;C&quot; | &quot;A&quot; &quot;B&quot; Refactor the grammar to extract common prefixes. myRule: &quot;A&quot; &quot;B&quot; (&quot;C&quot;)?","tags":"","url":"Building_Grammars\/resolving_grammar_errors.html"},{"title":"resolving lexer errors","text":"Resolving Lexer Errors No LINE_BREAKS Error. Unexpected RegExp Anchor Error. Token Can Never Be Matched. No LINE_BREAKS Error. A Chevrotain Lexer will by default track the full position information for each token. This includes line and column information. In order to support this the Lexer must be aware of which Tokens may include line terminators. This information must be provided by the lexer's author. This error means that the Lexer has been defined to track line and column information (perhaps by default). Yet not a single one of the Token definitions passed to it was defined as possibly containing line terminators. To resolve this choose one of the following: Disable the line and column position tracking using the positionTracking configuration option. const myTokens = [IntegerLiteral, StringLiteral, WhiteSpace \/*, ... *\/] const myLexer = new chevrotain.Lexer([myTokens], { positionTracking: &quot;onlyOffset&quot; }) Mark the Tokens which may include a line terminator with a line_breaks flag. const createToken = chevrotain.createToken \/\/ Using createToken API const Whitespace = createToken({ name: &quot;Whitespace&quot;, pattern: \/\\s+\/, line_breaks: true }) \/\/ or in ES2015 syntax with static properties class Whitespace extends chevrotain.Token {} Whitespace.PATTERN = \/\\s+\/ Whitespace.LINE_BREAKS = true const myTokens = [IntegerLiteral, StringLiteral, WhiteSpace \/*, ... *\/] const myLexer = new chevrotain.Lexer([myTokens]) Note that the definition of what constitutes a line terminator is controlled by the lineTerminatorsPattern lexer configuration property. Also note that multi-line tokens such as some types of comments and string literals tokens may contain line terminators, if your language includes such tokens they must also be marked with the line_breaks flag. Unexpected RegExp Anchor Error. A Token RegExp pattern used in a chevrotain lexer may not use the start\/end of input anchors ('$' and '^'). const createToken = chevrotain.createToken \/\/ Using createToken API const Whitespace = createToken({ name: &quot;Integer&quot;, \/\/ invalid pattern using both anchors pattern: \/^\\d+$\/ }) This will be checked for during the initialization of the lexer. Unfortunately, this validation can detect false positives when the anchor characters are used in certain regExp contexts, for example: const createToken = chevrotain.createToken const semVer = createToken({ name: &quot;semVer&quot;, \/\/ will match semantic versions such as: &quot;1.0.2&quot;, &quot;^0.3.9&quot; \/\/ inside a character set ([...]) the carat ('^') character does not act as an anchor. \/\/ yet it would still cause the validation to fail. pattern: \/[~^]?\\d+\\.\\d+\\.\\d+\/ }) \/\/ will throw an error new chevrotain.Lexer([semVer]) It is possible to workaround this problem by simply escaping the the offending carat or dollar sign. const semVer = createToken({ name: &quot;semVer&quot;, pattern: \/[~\\^]?\\d+\\.\\d+\\.\\d+\/ }) Token can never be matched. This error means that A Token type can never be successfully matched as a previous Token type in the lexer definition will always matched instead. This happens because the default behavior of Chevrotain is to attempt to match tokens by the order described in the lexer definition. For example: const ForKeyword = createToken({ name: &quot;ForKeyword&quot;, pattern: \/for\/ }) const Identifier = createToken({ name: &quot;Identifier&quot;, pattern: \/[a-zA-z]+\/ }) \/\/ Will throw Token &lt;ForKeyword&gt; can never be matched... \/\/ Because the input &quot;for&quot; is also a valid identifier \/\/ and matching an identifier will be attempted first. const myLexer = new chevrotain.Lexer([Identifier, ForKeyword]) Note that this validation is limited to simple patterns such as keywords The more general case of any pattern being a strict subset of a preceding pattern will require much more in depth RegExp analysis capabilities. To resolve this simply re-arrange the order of Token types in the lexer definition such that the more specific Token types will be listed first. \/\/ Identifier is now listed as the last Token type. const myLexer = new chevrotain.Lexer([ForKeyword, Identifier]) Note that the solution provided above will create a new problem. Any identifier starting with &quot;for&quot; will be lexed as two separate tokens, a ForKeyword and an identifier. For example: const myLexer = new chevrotain.Lexer([ForKeyword, Identifier]) \/\/ [ \/\/ {image:&quot;for&quot;} \/\/ {image:&quot;ward&quot;} \/\/ ] const tokensResult = myLexer.tokenize(&quot;forward&quot;) To resolve this second problem see how to prefer the longest match as demonstrated in the [keywords vs identifiers example][keywords_idents] [keywords_idents] https:\/\/github.com\/SAP\/Chevrotain\/blob\/master\/examples\/lexer\/keywords_vs_identifiers\/keywords_vs_identifiers.js","tags":"","url":"Building_Grammars\/resolving_lexer_errors.html"},{"title":"FAQ","text":"FAQ Why should I use a Parsing DSL instead of a Parser Generator? What Differentiates Chevrotain from other Parsing Libraries? Why are Error Recovery \/ Fault Tolerant capabilities needed in a Parser? How do I debug my parser? Why are the unique numerical suffixes (CONSUME1\/CONSUME2\/...) needed for the DSL Rules? Why does Chevrotain not work correctly after I minified my Sources? Why does Chevrotain not work correctly after I webpacked my Sources? Why does my parser appear to be stuck during it's initialization? How do I Maximize my parser's performance? Why should I use a Parsing DSL instead of a Parser Generator? A Parser Generator adds an (unnecessary) level of abstraction between the grammar implementation and the actual parser. This is because the grammar is written in a different language than the target runtime. Debugging a generated parser means looking at different code than the actual grammar specifications. This generated code is often huge, verbose and hard to understand. On the other hand, when debugging a Parser implemented using a Parsing DSL, The actual Grammar's code the implementer wrote(not generated code) is debugged. So debugging Chevrotain is just like debugging any other JavaScript code. No need to handle grammar generation as part of the build process or commit generated files to the source code. No need to learn a new syntax, as Chevrotain is a Pure JavasScript Library. instead the problem is reduced to learning a new API. No need for a special editor to write the Grammar, just use your favorite JavaScript editor. What Differentiates Chevrotain from other JavaScript Parsing Solutions? Performance: Chevrotain is generally faster (often much more so) than other existing JavaScript Parsing Solutions. And can even compete with the performance of hand built parsers. See an Online Benchmark that compares the performance of JSON Parsers implemented using multiple JavaScript Parsing solutions. Error Recovery \/ Fault Tolerance: With the exception of Antlr4, other JavaScript Parsing Solutions usually do not have Error Recovery capabilities. Why are Error Recovery \/ Fault Tolerant capabilities needed in a Parser? When building a standard compiler that should only handle completely valid inputs these capabilities are indeed irrelevant. But for the use case of building Editor Tools \/ Language Services the parser must be able to handle partially invalid inputs as well. Some examples: All syntax errors should be reported and not just the first one. Refactoring should work even if there is a missing comma somewhere. Autocomplete \/ Intellisense should work even if there is a syntax error prior to the requested suggestion position. How do I debug my parser? Just add a breakpoint in your favorites IDE and debug, same as you would for any other JavaScript code. Chevrotain Grammars are pure javascript code. No special handling required. Why are the unique numerical suffixes (CONSUME1\/CONSUME2\/...) needed for the DSL Rules? Lets look at an example first: this.RULE(&quot;someRule&quot;, function() { $.OPTION(function() { $.CONSUME(MyToken); }); $.OPTION2(function() { $.CONSUME(MyOtherToken); }); $.OPTION3(function() { $.CONSUME2(MyToken); }); }); As you can see this example uses three different variations of OPTION(1|2|3) and two variations of CONSUME(1|2). This is because during parsing runtime Chevrotain must be able to distinguish between the variations of the same Parsing rule. The combination of the DSL Rule(OPTION\/MANY\/CONSUME), the DSL Rule's optional numerical suffix and the DSL rule's parameter (if available) defines a unique key which Chevrotain uses to figure out the current location in the grammar. This location information is then used for many things such as: Computing the lookahead function which decides if a DSL rule should be entered or which alternatives should be taken. Computing an appropriate error message which includes the list of next valid possible tokens. Performing automatic Error Recovery by figuring out &quot;re-sync&quot; tokens. Why does Chevrotain not work correctly after I minified my Grammar? Chevrotain relies on Function.name property and Function.toString(). This means that certain aggressive minification options can break Chevrotain grammars. See related documentation for details &amp; workarounds. Why does Chevrotain not work correctly after I webpacked my Grammar? Chevrotain relies on Function.name property and Function.toString(). This means that certain aggressive webpack 2 optimizations (tree shaking) can break Chevrotain grammars under certain conditions. See related documentation for details &amp; workarounds. Why does my parser appear to be stuck during it's initialization? The first time a Chevrotain parser is initialized additional validations and computations are performed. Some of these can take a very long time under certain edge cases. Specifically the detection of ambiguous alternatives when the parser uses a larger than the default maxLookahead and there are many (thousands) of ambiguous paths. To resolve this try reducing the maxLookahead and inspect the ambiguity errors to fix the grammar ambiguity which is the root cause of the problem. How do I Maximize my parser's performance? Major Performance Benefits These are highly recommended for each and every parser. Do not create a new Parser instance for each new input. Instead re-use a single instance and reset its state between iterations. For example: \/\/ reuse the same parser instance. var parser = new JsonParserES5([]); module.exports = function (text) { var lexResult = JsonLexer.tokenize(text); \/\/ setting a new input will RESET the parser instance's state. parser.input = lexResult.tokens; var value = parser.json(); return { value: value, lexErrors: lexResult.errors, parseErrors: parser.errors }; }; This will avoid the fixed cost of reinitializing a parser instance. But more importantly this pattern seems to help V8 Engine to avoid de-optimizations. Such a pattern can lead to 15%-100% performance boost on V8 (Node.js\/Chrome) depending on the grammar used. Note that this means that if your parser &quot;carries&quot; additional state, that state should also be reset. Simply override the Parser's reset method to accomplish that. Avoid reinitializing large arrays of alternatives. The syntax for alternatives (OR) requires creating an array on every single invocation. For large enough arrays and in rules which are called often this can cause quite a large performance penalty. $.RULE(&quot;value&quot;, () =&gt; { $.OR( [ \/\/ an array with seven alternatives { ALT: () =&gt; { $.CONSUME(StringLiteral) }}, { ALT: () =&gt; { $.CONSUME(NumberLiteral) }}, { ALT: () =&gt; { $.SUBRULE($.object) }}, { ALT: () =&gt; { $.SUBRULE($.array) }}, { ALT: () =&gt; { $.CONSUME(True) }}, { ALT: () =&gt; { $.CONSUME(False) }}, { ALT: () =&gt; { $.CONSUME(Null) }} ]); }); A simple JavaScript pattern can avoid this costly re-initilization: $.RULE(&quot;value&quot;, function () { \/\/ c1 is used as a cache, the short circute &quot;||&quot; will ensure only a single initilization $.OR($.c1 || ($.c1 = [ { ALT: () =&gt; { $.CONSUME(StringLiteral) }}, { ALT: () =&gt; { $.CONSUME(NumberLiteral) }}, { ALT: () =&gt; { $.SUBRULE($.object) }}, { ALT: () =&gt; { $.SUBRULE($.array) }}, { ALT: () =&gt; { $.CONSUME(True) }}, { ALT: () =&gt; { $.CONSUME(False) }}, { ALT: () =&gt; { $.CONSUME(Null) }} ])); }); Applying this pattern (in just a single location) on a JSON grammar provided 25-30% performance boost (Node.js 8), For a CSS grammar (2 locations) this resulted in about 20% speed boost. It is important to note that: This pattern should only be applied on largish number of alternatives, testing on node.js 8.0 showed it was only useful when there are at least four alternatives. In cases with fewer alternatives this pattern would actually be slower! This pattern can only be applied if there are no vars which can change accessed via closures. Example: \/\/ BAD $.RULE(&quot;value&quot;, function () { let result \/\/ We reference the &quot;result&quot; variable via a closure. \/\/ So a new function is needed each time this grammar rule is invoked. $.OR($.c1 || ($.c1 = [ { ALT: () =&gt; { result = $.CONSUME(StringLiteral) }}, ]))}) \/\/ GOOD $.RULE(&quot;value&quot;, function () { let result \/\/ no closure for the result variable, we use the returned value of the OR instead. result = $.OR($.c1 || ($.c1 = [ { ALT: () =&gt; { return $.CONSUME(StringLiteral) }}, ]))}) Note that gates \/ predicaetes often use vars from closures. Due to the way Chevrotain is built, the text of the alternatives cannot be completly extracted from the grammar rule let myAlts = [ { ALT: () =&gt; { return $.CONSUME(StringLiteral) }}, ] \/\/ Won't work $.RULE(&quot;value&quot;, function () { \/\/ Chevrotain won't be able to analyze this grammar rule as it relies on Function.prototype.toString result = $.OR(myAlts); }) Avoid dynamically changing the parser instance. The line: &quot;$.c1 || ($.c1 = ...&quot; ($ is 'this') Will cause a 'c1' property to be assigned to the parser instance. This may seem innocent but if enough properties are added dynamically to an instance its V8 hidden class will change which could cause a severe performance reduction. To avoid this, simpliy define these &quot;cache properties&quot; in the constructor. See an example in the ECMAScript5 grammar's constructor. Minor Performance Benefits These are only required if you are trying to squeeze every tiny bit of performance out of your parser. Reduce the amount of Token position tracking the lexer performs. See The ILexerConfig.positionTracking property. Avoid creating parsing rules which only parse a single Terminal. There is a certain fixed overhead for the invocation of each parsing rule. Normally there is no reason to pay it for a Rule which only consumes a single Terminal. For example: this.myRedundantRule = this.RULE(&quot;myRedundantRule&quot;, function() { $.CONSUME(StringLiteral); }); Instead such a rule's contents should be (manually) in-lined in its call sites. *Avoid _SEP DSL methods (MANY_SEP \/ AT_LEAST_ONE_SEP). The *_SEP DSL methods also collect the separator Tokens parsed. Creating these arrays has a small overhead (several percentage). Which is a complete waste in most cases where those separators tokens are not needed for any output data structure. Use the returned values of iteration DSL methods (MANY\/MANY_SEP\/AT_LEAST_ONE\/AT_LEAST_ONE_SEP). Consider the following grammar rule: this.RULE(&quot;array&quot;, function() { let myArr = [] $.CONSUME(LSquare); values.push($.SUBRULE($.value)); $.MANY(() =&gt; { $.CONSUME(Comma); values.push($.SUBRULE2($.value)); }); $.CONSUME(RSquare); }); The values of the array are manually collected inside the &quot;myArr&quot; array. However another result array is already created by invoking the iteration DSL method &quot;MANY&quot; This is obviously a waste of cpu cycles... A slightly more efficient (but syntactically ugly) alternative would be: this.RULE(&quot;array&quot;, function() { let myArr = [] $.CONSUME(LSquare); values.push($.SUBRULE($.value)); let iterationResult = $.MANY(() =&gt; { $.CONSUME(Comma); return $.SUBRULE2($.value); }); myArr = myArr.concat(iterationResult) $.CONSUME(RSquare); });","tags":"","url":"FAQ.html"},{"title":"Roadmap","text":"RoadMap Version 3.0 The main focus for version 3.0 will be to upgrade the CST related capabilities. This could mean many things: CST customization support. Skipping nodes in the created CST. Node labeling Performance optimizations for CST creation \/ Visitor. ... The objective is to make the CST and its utilities as user friendly and performant as possible So there would be very little reason to use embedded actions. ~~Version 2.0~~ The main focus of version 2.0 will be the addition of support for custom declarative style APIs. Either Generator EBNF style API, Combinator Style APIs or even both. This will enable Chevrotain to be used as the underlying engine for other parsing libraries. ~~Version 1.0~~ The APIs have been pretty stable for a while so Version 1.0 is in the works. This release will focus on improving the docs, adding a few more example grammars and minor fixes.","tags":"","url":"Roadmap.html"}]}